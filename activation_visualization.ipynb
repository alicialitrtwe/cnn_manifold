{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"activation_visualization.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1vJToSAKOnuiqiXZnt9uo3c1au9J8A99t","authorship_tag":"ABX9TyMRG1UAjFudmQLJoEeQL01O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"uszMhRsgRzEd","executionInfo":{"status":"ok","timestamp":1614892665204,"user_tz":480,"elapsed":2330,"user":{"displayName":"Alicia Zeng PhD","photoUrl":"","userId":"02959205937062628571"}}},"source":["import math\n","#%tensorflow_version 1.x\n","import tensorflow as tf\n","import numpy as np\n","import json\n","import tensorflow_datasets as tfds\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.applications.resnet_v2 import preprocess_input\n","from tensorflow.keras.applications.resnet_v2 import decode_predictions\n","from IPython.display import Image, display\n","from IPython.display import Image, display\n","import keras"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FJ2sV2vpD9rd","executionInfo":{"status":"ok","timestamp":1614893155175,"user_tz":480,"elapsed":306,"user":{"displayName":"Alicia Zeng PhD","photoUrl":"","userId":"02959205937062628571"}},"outputId":"9b5c86de-98e0-49e6-8d02-e266c9b15e22"},"source":["%cd /content/drive/MyDrive/cnn_embed/cnn_manifold"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/cnn_embed/cnn_manifold\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IUOae4c3FDoS","executionInfo":{"status":"ok","timestamp":1614893158152,"user_tz":480,"elapsed":2250,"user":{"displayName":"Alicia Zeng PhD","photoUrl":"","userId":"02959205937062628571"}}},"source":["!git config --global user.email \"aliciaxiaozeng@gmail.com\"\n","!git config --global user.name \"alicialitrtwe\""],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MYXwPphBE3eY","executionInfo":{"status":"ok","timestamp":1614893167733,"user_tz":480,"elapsed":7908,"user":{"displayName":"Alicia Zeng PhD","photoUrl":"","userId":"02959205937062628571"}},"outputId":"0ff1c45d-8b3a-4e05-fae6-53afe35cd519"},"source":["!git commit -m 'visualize for any direction'"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[main 0a26a33] visualize for any direction\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite activation_visualization.ipynb (99%)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ghVWxnZ5iqgO","executionInfo":{"status":"ok","timestamp":1614892202295,"user_tz":480,"elapsed":457,"user":{"displayName":"Alicia Zeng PhD","photoUrl":"","userId":"02959205937062628571"}}},"source":["# TODO proper reception field size\n","# TODO image initiation does not work for inception deeper layers\n","# TODO optimization steps"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"fqoRvmYmSTsU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614892673827,"user_tz":480,"elapsed":7654,"user":{"displayName":"Alicia Zeng PhD","photoUrl":"","userId":"02959205937062628571"}},"outputId":"1271f33f-ed8d-487c-95f5-2f11f662d743"},"source":["layer_name = \"conv3_block4_out\"\n","model = tf.keras.applications.ResNet50V2(weights=\"imagenet\", include_top=False)\n","#layer_name = \"mixed3\"\n","#model = tf.keras.applications.InceptionV3(weights=\"imagenet\", include_top=False)\n","layer = model.get_layer(name=layer_name)\n","activation_model = tf.keras.models.Model(inputs=model.inputs, outputs=layer.output)\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94674944/94668760 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OwYf605eTIKi","executionInfo":{"status":"ok","timestamp":1614892673831,"user_tz":480,"elapsed":7027,"user":{"displayName":"Alicia Zeng PhD","photoUrl":"","userId":"02959205937062628571"}}},"source":["def initialize_image(img_size):\n","    # We start from a gray image with some random noise\n","    img = tf.Variable(tf.random.normal([1, img_size, img_size, 3], 0, 0.01, tf.float32, seed=1))\n","    # ResNet50V2 expects inputs in the range [-1, +1].\n","    return img\n","\n","def compute_loss(img, direction, activation_model, img_size=180, cos_sim=False, x=None, y=None):\n","    #TODO: batch\n","    acts = activation_model(img) \n","    # visualize the neuron in the center of each layer\n","    shape = tf.shape(acts)\n","    x_ = shape[1] // 2 if x is None else x\n","    y_ = shape[2] // 2 if y is None else y\n","    filter_activation = acts[0, 2:-2, 2:-2, :]\n","    # TODO: cosine similarity\n","    if cos_sim==False:\n","        loss = -tf.reduce_mean(filter_activation*direction)\n","    return loss\n","def gradient_ascent_step(img, direction, activation_model, learning_rate, opt):\n","    with tf.GradientTape() as tape:\n","        tape.watch(img)\n","        loss = compute_loss(img, direction, activation_model)\n","    # Compute gradients.\n","    grads = tape.gradient(loss, img)\n","    # Normalize gradients.\n","    grads = tf.math.l2_normalize(grads)\n","    opt.apply_gradients(zip([grads], [img]))\n","    loss = compute_loss(img, direction, activation_model)\n","    return loss, img\n","def visualize_filter(img, direction, activation_model, img_size=180, learning_rate=0.1, n_iter=100):\n","    \n","    # gradient ascent\n","    #img = initialize_image(img_size)\n","    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    for i in range(n_iter):\n","        loss, img = gradient_ascent_step(img, direction, activation_model, learning_rate, opt)\n","        # Progress\n","        if (i % 100 == 0):\n","            print(f'i = {i}')\n","    # Decode the resulting input image\n","    processed_img = deprocess_image(img[0].numpy())\n","    return loss, img, processed_img\n","def deprocess_image(img):\n","    # Normalize array: center on 0., ensure variance is 0.15\n","    img -= img.mean()\n","    img /= img.std() + 1e-5\n","    img *= 0.15\n","\n","    # Center crop\n","    img = img[25:-25, 25:-25, :]\n","\n","    # Clip to [0, 1]\n","    img += 0.5\n","    img = np.clip(img, 0, 1)\n","\n","    # Convert to RGB array\n","    img *= 255\n","    img = np.clip(img, 0, 255).astype(\"uint8\")\n","    return img"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"sMwA96r-2fph","executionInfo":{"status":"ok","timestamp":1614892673832,"user_tz":480,"elapsed":2800,"user":{"displayName":"Alicia Zeng PhD","photoUrl":"","userId":"02959205937062628571"}}},"source":["img_size = 180"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"gBzoc5Wl4BQ9","executionInfo":{"status":"ok","timestamp":1614892679536,"user_tz":480,"elapsed":293,"user":{"displayName":"Alicia Zeng PhD","photoUrl":"","userId":"02959205937062628571"}}},"source":["directions = np.random.random((4, 512))"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nnNABz8wF3Wy","executionInfo":{"status":"ok","timestamp":1614892707994,"user_tz":480,"elapsed":319,"user":{"displayName":"Alicia Zeng PhD","photoUrl":"","userId":"02959205937062628571"}},"outputId":"f212abb5-860d-430f-cbc4-d9bdb89d05f4"},"source":["len(directions)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N6khNUiTi8hR","executionInfo":{"status":"ok","timestamp":1614893011004,"user_tz":480,"elapsed":1290,"user":{"displayName":"Alicia Zeng PhD","photoUrl":"","userId":"02959205937062628571"}},"outputId":"e9c341d7-ae6b-4846-adc0-f45271898e91"},"source":["%%time\n","loss_list = []\n","visualization_list = []\n","for i in range(len(directions)):\n","    img = initialize_image(img_size)\n","    loss, img, processed_img = visualize_filter(img, directions[i], activation_model, n_iter=4, learning_rate=100)\n","    loss_list.append(loss)\n","    visualization_list.append(img)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["i = 0\n","i = 0\n","i = 0\n","i = 0\n","CPU times: user 982 ms, sys: 7.49 ms, total: 989 ms\n","Wall time: 987 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HqHVsC15E101"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"itl92ef-Ez5z"},"source":["keras.preprocessing.image.save_img(\"0.png\", processed_img)\n","display(Image(\"0.png\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ouAKnZ63rjbT","executionInfo":{"status":"ok","timestamp":1614839473121,"user_tz":480,"elapsed":506,"user":{"displayName":"Alicia Zeng PhD","photoUrl":"","userId":"02959205937062628571"}}},"source":["# TODO tf.function\n","def compute_loss(input_image, direction_index, activation_model):\n","    activation = activation_model(input_image)\n","    # We avoid border artifacts by only involving non-border pixels in the loss.\n","    filter_activation = activation[:, 2:-2, 2:-2, direction_index]\n","    return -tf.reduce_mean(filter_activation)\n","\n","def initialize_image(img_size):\n","    # We start from a gray image with some random noise\n","    img = tf.Variable(tf.random.normal([1, img_size, img_size, 3], 0, 0.01, tf.float32, seed=1))\n","    # ResNet50V2 expects inputs in the range [-1, +1].\n","    return img\n","\n","\n","def gradient_ascent_step_Adam(img, direction_index, activation_model, learning_rate, opt):\n","    with tf.GradientTape() as tape:\n","        tape.watch(img)\n","        loss = compute_loss(img, direction_index, activation_model)\n","    # Compute gradients.\n","    grads = tape.gradient(loss, img)\n","    # Normalize gradients.\n","    grads = tf.math.l2_normalize(grads)\n","    opt.apply_gradients(zip([grads], [img]))\n","    loss = compute_loss(img, direction_index, activation_model)\n","    return loss, img\n","\n","def visualize_filter_Adam(img, direction_index, activation_model, img_size=180, learning_rate=0.1, n_iter=100):\n","    # gradient ascent\n","    #img = initialize_image(img_size)\n","    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    for i in range(n_iter):\n","        loss, img = gradient_ascent_step_Adam(img, direction_index, activation_model, learning_rate, opt)\n","        # Progress\n","        if (i % 100 == 0):\n","            print(i)\n","    # Decode the resulting input image\n","    processed_img = deprocess_image(img[0].numpy())\n","    return loss, img, processed_img\n","\n","def deprocess_image(img):\n","    # Normalize array: center on 0., ensure variance is 0.15\n","    img -= img.mean()\n","    img /= img.std() + 1e-5\n","    img *= 0.15\n","\n","    # Center crop\n","    img = img[25:-25, 25:-25, :]\n","\n","    # Clip to [0, 1]\n","    img += 0.5\n","    img = np.clip(img, 0, 1)\n","\n","    # Convert to RGB array\n","    img *= 255\n","    img = np.clip(img, 0, 255).astype(\"uint8\")\n","    return img"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"id":"-g3Al_Lqc4Mc"},"source":["all_imgs = []\n","for filter_index in range(64):\n","    print(\"Processing filter %d\" % (filter_index,))\n","    loss, img = visualize_filter(filter_index)\n","    all_imgs.append(img)\n","\n","# Build a black picture with enough space for\n","# our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n","margin = 5\n","n = 8\n","cropped_width = img_width - 25 * 2\n","cropped_height = img_height - 25 * 2\n","width = n * cropped_width + (n - 1) * margin\n","height = n * cropped_height + (n - 1) * margin\n","stitched_filters = np.zeros((width, height, 3))\n","\n","# Fill the picture with our saved filters\n","for i in range(n):\n","    for j in range(n):\n","        img = all_imgs[i * n + j]\n","        stitched_filters[\n","            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,\n","            (cropped_height + margin) * j : (cropped_height + margin) * j\n","            + cropped_height,\n","            :,\n","        ] = img\n","keras.preprocessing.image.save_img(\"stiched_filters.png\", stitched_filters)\n","\n","from IPython.display import Image, display\n","\n","display(Image(\"stiched_filters.png\"))"],"execution_count":null,"outputs":[]}]}